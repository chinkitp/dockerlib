{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@6e62ca2c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://b9ee67753860:4040)\" target=\"new_tab\">Spark UI: local-1536705661365</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1536705661365: Some(http://b9ee67753860:4040)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.{Dataset, Row, DataFrame, Column, DataFrameWriter, SaveMode}\n",
    "import org.apache.spark.sql.functions.{concat, lit,concat_ws}\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asArrayDelimited: (c: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def asArrayDelimited(c: Column) = concat(concat_ws(\"|\", c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MyPimpedDataFrameWriter3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Dataset, Row, DataFrame, Column, DataFrameWriter, SaveMode}\n",
    "implicit class MyPimpedDataFrameWriter3[T](dfw: DataFrameWriter[T]) {\n",
    "    \n",
    "    def saveAsCsv3(path: String): Unit = {\n",
    "        dfw            \n",
    "            .option(\"delimiter\",\"\\t\")   \n",
    "            .option(\"quote\",\"\"\"\"\"\")   \n",
    "            .format(\"csv\") \n",
    "            .mode(SaveMode.Overwrite)\n",
    "            .save(path)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV for Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfCompanyNames = [data: struct<value: string>, id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[data: struct<value: string>, id: string ... 1 more field]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfCompanyNames = spark.read.json(\"/abr/vertices/company-names/*.txt\")\n",
    "\n",
    "dfCompanyNames\n",
    "    .select(\"id\",\"data.value\",\"meta.graphs\")\n",
    "    .withColumn(\"graphs\",asArrayDelimited($\"graphs\"))\n",
    "    .write\n",
    "    .saveAsCsv3(\"/abr/vertices.csv/company-names/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfBusinessNames = [data: struct<value: string>, id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[data: struct<value: string>, id: string ... 1 more field]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfBusinessNames = spark.read.json(\"/abr/vertices/business-names/*.txt\")\n",
    "\n",
    "/*dfBusinessNames.filter($\"id\".contains(\"\\\"\")).select(\"id\").write.saveAsCsv(\"/abr/string.issues/\")*/\n",
    "\n",
    "dfBusinessNames\n",
    "    .select(\"id\",\"data.value\",\"meta.graphs\")\n",
    "    .withColumn(\"graphs\",asArrayDelimited($\"graphs\"))\n",
    "    .write \n",
    "    .saveAsCsv3(\"/abr/vertices.csv/business-names/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfABN = [data: struct<abn: string, abnStatus: string ... 6 more fields>, id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[data: struct<abn: string, abnStatus: string ... 6 more fields>, id: string ... 1 more field]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfABN = spark.read.json(\"/abr/vertices/{abn-abr,asic-abn-without-an-abr}/*.txt\")\n",
    "\n",
    "dfABN\n",
    "    .select(\"id\",\n",
    "             \"data.abn\",\n",
    "             \"data.abnStatus\",\n",
    "             \"data.entityType\",\n",
    "             \"data.entityTypeDescription\",\n",
    "             \"data.gstStatus\",\n",
    "             \"data.gstStatusFrom\",\n",
    "             \"data.value\",\n",
    "             \"meta.graphs\")\n",
    "    .withColumn(\"graphs\",asArrayDelimited($\"graphs\"))\n",
    "    .write\n",
    "    .saveAsCsv(\"/abr/vertices.csv/abn/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfACN = [data: struct<value: string>, id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[data: struct<value: string>, id: string ... 1 more field]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfACN = spark.read.json(\"/abr/vertices/acn-abr-and-company-names/*.txt\")\n",
    "\n",
    "dfACN\n",
    "    .select(\"id\",\"data.value\",\"meta.graphs\")\n",
    "    .withColumn(\"graphs\",asArrayDelimited($\"graphs\"))\n",
    "    .write\n",
    "    .saveAsCsv3(\"/abr/vertices.csv/acn/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfLegalEntity = spark.read.json(\"/abr/vertices/legal-entity/*.txt\")\n",
    "\n",
    "dfLegalEntity\n",
    "    .select(\"id\",\n",
    "            \"data.address.postCode\",\n",
    "            \"data.address.state\",\n",
    "            \"data.familyName\",\n",
    "            \"data.givenNames\",\n",
    "            \"data.title\",\n",
    "            \"data.type\",\n",
    "            \"meta.graphs\"\n",
    "           )\n",
    "    .withColumn(\"graphs\",asArrayDelimited($\"graphs\"))\n",
    "    .withColumn(\"givenNames\",asArrayDelimited($\"givenNames\"))\n",
    "    .write\n",
    "    .saveAsCsv(\"/abr/vertices.csv/legal-entity/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfMainEntity = spark.read.json(\"/abr/vertices/main-entity/*.txt\")\n",
    "\n",
    "dfMainEntity\n",
    "    .select(\"id\",\n",
    "            \"data.address.postCode\",\n",
    "            \"data.address.state\",\n",
    "            \"data.nonIndividualName\",\n",
    "            \"data.type\",\n",
    "            \"meta.graphs\"\n",
    "           )\n",
    "    .withColumn(\"graphs\",asArrayDelimited($\"graphs\"))\n",
    "    .write\n",
    "    .saveAsCsv(\"/abr/vertices.csvmain-entity/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfOtherEntity = [data: struct<nonIndividualName: string, type: string>, id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[data: struct<nonIndividualName: string, type: string>, id: string ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfOtherEntity = spark.read.json(\"/abr/vertices/other-entity/*.txt\")\n",
    "\n",
    "dfOtherEntity\n",
    "    .select(\"id\",\n",
    "            \"data.nonIndividualName\",\n",
    "            \"data.type\",\n",
    "            \"meta.graphs\"\n",
    "           )\n",
    "    .withColumn(\"graphs\",asArrayDelimited($\"graphs\"))\n",
    "    .write.format(\"csv\").save(\"/abr/vertices.csv/other-entity/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
