{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://central.maven.org/maven2/org/bitcoinj/bitcoinj-core/0.14.7/bitcoinj-core-0.14.7-bundled.jar\n",
      "Finished download of bitcoinj-core-0.14.7-bundled.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://central.maven.org/maven2/org/bitcoinj/bitcoinj-core/0.14.7/bitcoinj-core-0.14.7-bundled.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.bitcoinj.core.{Block, Context, Transaction, Utils}\n",
    "import org.bitcoinj.params.MainNetParams\n",
    "import scala.util.{Failure, Success, Try}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isMagic: (bytes: Array[Byte], startSeq: Int)Boolean\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def isMagic(bytes: Array[Byte], startSeq: Int): Boolean= {\n",
    "            if ((bytes.size != startSeq)\n",
    "                && (bytes(startSeq) & 0xFF) == 0xF9\n",
    "                && (bytes(startSeq + 1) & 0xFF) == 0xbe\n",
    "                && (bytes(startSeq + 2) & 0xFF) == 0xb4\n",
    "                && (bytes(startSeq + 3) & 0xFF) == 0xd9)\n",
    "                return true\n",
    "\n",
    "            false\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toBlockBytes: (t: Array[Byte])Seq[Array[Byte]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toBlockBytes(t: Array[Byte]) : Seq[Array[Byte]] = {\n",
    "    var blocksAsBytes = Seq.newBuilder[Array[Byte]]\n",
    "    var byteCursor = 0\n",
    "    var blockCount = 0\n",
    "\n",
    "    while (isMagic(t, byteCursor)) {\n",
    "\n",
    "        blockCount = blockCount + 1;\n",
    "\n",
    "        var blockSizeBytes = new Array[Byte](4)\n",
    "        System.arraycopy(t, byteCursor + 4, blockSizeBytes,0,4)\n",
    "        val blockSize = Utils.readUint32BE(Utils.reverseBytes(blockSizeBytes), 0)\n",
    "        var blockBytes = new Array[Byte](blockSize.toInt)\n",
    "        System.arraycopy(t,byteCursor + 8,blockBytes,0, blockSize.toInt);\n",
    "        byteCursor = byteCursor + 8 + blockSize.toInt;\n",
    "        blocksAsBytes += blockBytes\n",
    "    }    \n",
    "    \n",
    "    blocksAsBytes.result()\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:1: error: ':' expected but ')' found.\n",
       "def g(x) = {\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "files = MapPartitionsRDD[7] at map at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at map at <console>:42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var files = sc.binaryFiles(\"/bitcoin-blocks/blk0000*.dat\")\n",
    "                .map(f => (f._1,f._2.toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blocksAsBytes = MapPartitionsRDD[6] at map at <console>:53\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[6] at map at <console>:53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var blocksAsBytes = files.map(f => (f._1, Try(toBlockBytes(f._2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "failed = MapPartitionsRDD[14] at distinct at <console>:57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[14] at distinct at <console>:57"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var failed = blocksAsBytes.filter(f => f._2.isFailure)\n",
    "                .map(f => f._1)\n",
    "                .distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filesSizes = MapPartitionsRDD[5] at map at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at map at <console>:43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var filesSizes = files.map(f => f._2.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"Executor task launch worker for task 2\" java.lang.SecurityException: Not allowed to invoke System.exit!\n",
      "\tat org.apache.toree.security.KernelSecurityManager.checkExit(KernelSecurityManager.scala:147)\n",
      "\tat java.lang.Runtime.halt(Runtime.java:273)\n",
      "\tat org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:32)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
       "\tat java.util.Arrays.copyOf(Arrays.java:3236)\n",
       "\tat java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:191)\n",
       "\tat org.spark_project.guava.io.ByteStreams.toByteArray(ByteStreams.java:253)\n",
       "\tat org.apache.spark.input.PortableDataStream.toArray(PortableDataStream.scala:193)\n",
       "\tat $line50.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:42)\n",
       "\tat $line50.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:42)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1838)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat java.util.Arrays.copyOf(Arrays.java:3236)\n",
       "\tat java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:191)\n",
       "\tat org.spark_project.guava.io.ByteStreams.toByteArray(ByteStreams.java:253)\n",
       "\tat org.apache.spark.input.PortableDataStream.toArray(PortableDataStream.scala:193)\n",
       "\tat $anonfun$1.apply(<console>:42)\n",
       "\tat $anonfun$1.apply(<console>:42)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1838)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1162)\n",
       "  ... 44 elided\n",
       "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
       "  at java.util.Arrays.copyOf(Arrays.java:3236)\n",
       "  at java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:191)\n",
       "  at org.spark_project.guava.io.ByteStreams.toByteArray(ByteStreams.java:253)\n",
       "  at org.apache.spark.input.PortableDataStream.toArray(PortableDataStream.scala:193)\n",
       "  at $anonfun$1.apply(<console>:42)\n",
       "  at $anonfun$1.apply(<console>:42)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1838)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
